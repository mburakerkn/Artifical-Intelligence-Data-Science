{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6169a3",
   "metadata": {},
   "source": [
    "# Big Data Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5f85dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "import collections\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6ab1d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Purchase</th>\n",
       "      <th>Account_Manager</th>\n",
       "      <th>Years</th>\n",
       "      <th>Num_Sites</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11066.80</td>\n",
       "      <td>0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>11916.22</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12884.75</td>\n",
       "      <td>0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8010.76</td>\n",
       "      <td>0</td>\n",
       "      <td>6.71</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9191.58</td>\n",
       "      <td>0</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Age  Total_Purchase  Account_Manager  Years  Num_Sites  Churn\n",
       "0           0  42.0        11066.80                0   7.22        8.0      1\n",
       "1           1  41.0        11916.22                0   6.50       11.0      1\n",
       "2           2  38.0        12884.75                0   6.67       12.0      1\n",
       "3           3  42.0         8010.76                0   6.71       10.0      1\n",
       "4           4  37.0         9191.58                0   5.56        9.0      1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yeni.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7abb7031",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ee5d542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35217669",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06b11640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA(n_components=1)\n",
    "x_r=pca.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1e9111f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999926772666408"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "73e9c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac040403",
   "metadata": {},
   "outputs": [],
   "source": [
    "deneme = d.append(v,ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "86345e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.merge(v, d,left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "94370be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1003.975939</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1853.396024</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2821.926301</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052.064051</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>871.243577</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  Churn\n",
       "0    -1003.975939    NaN\n",
       "1    -1853.396024    NaN\n",
       "2    -2821.926301    NaN\n",
       "3     2052.064051    NaN\n",
       "4      871.243577    NaN\n",
       "...           ...    ...\n",
       "1795          NaN    0.0\n",
       "1796          NaN    0.0\n",
       "1797          NaN    0.0\n",
       "1798          NaN    0.0\n",
       "1799          NaN    0.0\n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "478bbaa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Churn</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-2737.995945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>NaN</td>\n",
       "      <td>168.904999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1993.355666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3544.894889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>NaN</td>\n",
       "      <td>747.223764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Churn            0\n",
       "0      1.0          NaN\n",
       "1      1.0          NaN\n",
       "2      1.0          NaN\n",
       "3      1.0          NaN\n",
       "4      1.0          NaN\n",
       "..     ...          ...\n",
       "895    NaN -2737.995945\n",
       "896    NaN   168.904999\n",
       "897    NaN -1993.355666\n",
       "898    NaN  3544.894889\n",
       "899    NaN   747.223764\n",
       "\n",
       "[1800 rows x 2 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eb0b045a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1003.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1853.396024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2821.926301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2052.064051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>871.243577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>-2737.995945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>168.904999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>-1993.355666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>3544.894889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>747.223764</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "0   -1003.975939\n",
       "1   -1853.396024\n",
       "2   -2821.926301\n",
       "3    2052.064051\n",
       "4     871.243577\n",
       "..           ...\n",
       "895 -2737.995945\n",
       "896   168.904999\n",
       "897 -1993.355666\n",
       "898  3544.894889\n",
       "899   747.223764\n",
       "\n",
       "[900 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85c7299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv(\"big.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8710a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"np.txt\", r.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b67b3477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Purchase</th>\n",
       "      <th>Account_Manager</th>\n",
       "      <th>Years</th>\n",
       "      <th>Num_Sites</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.0</td>\n",
       "      <td>11066.80</td>\n",
       "      <td>0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.0</td>\n",
       "      <td>11916.22</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>12884.75</td>\n",
       "      <td>0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>8010.76</td>\n",
       "      <td>0</td>\n",
       "      <td>6.71</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.0</td>\n",
       "      <td>9191.58</td>\n",
       "      <td>0</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Total_Purchase  Account_Manager  Years  Num_Sites\n",
       "0  42.0        11066.80                0   7.22        8.0\n",
       "1  41.0        11916.22                0   6.50       11.0\n",
       "2  38.0        12884.75                0   6.67       12.0\n",
       "3  42.0         8010.76                0   6.71       10.0\n",
       "4  37.0         9191.58                0   5.56        9.0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24fcae84",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c7074f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df[\"Unnamed: 0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "002696be",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Names'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/883458436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4046\u001b[0m             \u001b[0;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4047\u001b[0m             \u001b[0;31m# exception:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4048\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4049\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3622\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3623\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3624\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3625\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Names'"
     ]
    }
   ],
   "source": [
    "del df[\"Names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8174f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Age              900 non-null    float64\n",
      " 1   Total_Purchase   900 non-null    float64\n",
      " 2   Account_Manager  900 non-null    int64  \n",
      " 3   Years            900 non-null    float64\n",
      " 4   Num_Sites        900 non-null    float64\n",
      " 5   Churn            900 non-null    int64  \n",
      "dtypes: float64(4), int64(2)\n",
      "memory usage: 42.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "38d30608",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_csv(\"big_son.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf6889e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"son.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ed670e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Age</th>\n",
       "      <th>Total_Purchase</th>\n",
       "      <th>Account_Manager</th>\n",
       "      <th>Years</th>\n",
       "      <th>Num_Sites</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>11066.80</td>\n",
       "      <td>0</td>\n",
       "      <td>7.22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>11916.22</td>\n",
       "      <td>0</td>\n",
       "      <td>6.50</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>38.0</td>\n",
       "      <td>12884.75</td>\n",
       "      <td>0</td>\n",
       "      <td>6.67</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8010.76</td>\n",
       "      <td>0</td>\n",
       "      <td>6.71</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>9191.58</td>\n",
       "      <td>0</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Age  Total_Purchase  Account_Manager  Years  Num_Sites  Churn\n",
       "0           0  42.0        11066.80                0   7.22        8.0      1\n",
       "1           1  41.0        11916.22                0   6.50       11.0      1\n",
       "2           2  38.0        12884.75                0   6.67       12.0      1\n",
       "3           3  42.0         8010.76                0   6.71       10.0      1\n",
       "4           4  37.0         9191.58                0   5.56        9.0      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2215625a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 21:07:49 WARN Utils: Your hostname, Mehmet-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.37 instead (on interface en0)\n",
      "22/08/31 21:07:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/31 21:07:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.0 1\n",
      "25.0 1\n",
      "26.0 2\n",
      "27.0 1\n",
      "28.0 5\n",
      "29.0 9\n",
      "30.0 11\n",
      "31.0 11\n",
      "32.0 18\n",
      "33.0 16\n",
      "34.0 25\n",
      "35.0 32\n",
      "36.0 39\n",
      "37.0 48\n",
      "38.0 51\n",
      "39.0 48\n",
      "40.0 58\n",
      "41.0 69\n",
      "42.0 49\n",
      "43.0 59\n",
      "44.0 53\n",
      "45.0 56\n",
      "46.0 48\n",
      "47.0 29\n",
      "48.0 36\n",
      "49.0 30\n",
      "50.0 15\n",
      "51.0 21\n",
      "52.0 20\n",
      "53.0 8\n",
      "54.0 8\n",
      "55.0 14\n",
      "56.0 5\n",
      "58.0 2\n",
      "60.0 1\n",
      "65.0 1\n",
      "Age 1\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Big Data ML Class.\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"churn.csv\")\n",
    "ratings = lines.map(lambda x: x.split(\",\")[2])\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2bed2dc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 1\n",
      "10005.91 1\n",
      "10017.82 1\n",
      "10026.44 1\n",
      "10028.23 1\n",
      "10041.13 1\n",
      "10050.61 1\n",
      "10054.6 1\n",
      "10056.55 1\n",
      "10058.87 1\n",
      "10074.4 1\n",
      "10076.71 1\n",
      "10110.4 1\n",
      "10133.36 1\n",
      "10147.22 1\n",
      "10157.21 1\n",
      "10166.67 1\n",
      "10179.92 1\n",
      "10180.59 1\n",
      "10182.6 1\n",
      "10183.98 1\n",
      "10199.44 1\n",
      "10203.18 1\n",
      "10216.59 1\n",
      "10226.43 1\n",
      "10236.95 1\n",
      "10241.32 1\n",
      "10268.0 1\n",
      "10268.87 1\n",
      "10269.97 1\n",
      "10271.19 1\n",
      "10276.33 1\n",
      "10290.14 1\n",
      "10292.19 1\n",
      "10298.29 1\n",
      "10300.24 1\n",
      "10306.21 1\n",
      "10309.15 1\n",
      "10309.71 1\n",
      "10313.06 1\n",
      "10314.67 1\n",
      "10323.07 1\n",
      "10328.42 1\n",
      "10332.16 1\n",
      "10338.09 1\n",
      "10339.05 1\n",
      "10345.19 1\n",
      "10348.37 1\n",
      "10356.02 1\n",
      "10358.08 1\n",
      "10363.63 1\n",
      "10364.82 1\n",
      "10367.08 1\n",
      "10367.14 1\n",
      "10370.72 1\n",
      "10375.39 1\n",
      "10388.38 1\n",
      "10399.19 1\n",
      "10400.88 1\n",
      "10406.58 1\n",
      "10413.93 1\n",
      "10427.12 1\n",
      "10430.5 1\n",
      "10442.37 1\n",
      "10445.76 1\n",
      "10448.09 1\n",
      "10466.56 1\n",
      "10469.5 1\n",
      "10474.94 1\n",
      "10484.33 1\n",
      "10491.4 1\n",
      "10494.82 1\n",
      "10500.77 1\n",
      "10510.95 1\n",
      "10522.21 1\n",
      "10522.6 1\n",
      "10526.18 1\n",
      "10527.52 1\n",
      "10528.71 1\n",
      "10530.33 1\n",
      "10550.87 1\n",
      "10565.59 1\n",
      "10578.14 1\n",
      "10583.76 1\n",
      "10584.71 1\n",
      "10585.86 1\n",
      "10598.35 1\n",
      "10606.97 1\n",
      "10617.16 1\n",
      "10631.84 1\n",
      "10638.65 1\n",
      "10638.66 1\n",
      "10643.67 1\n",
      "10661.48 1\n",
      "10668.91 1\n",
      "10674.92 1\n",
      "10675.86 1\n",
      "10697.72 1\n",
      "10709.39 1\n",
      "10716.75 1\n",
      "10718.97 1\n",
      "10741.9 1\n",
      "10744.14 1\n",
      "10746.37 1\n",
      "10746.69 1\n",
      "10762.29 1\n",
      "10770.82 1\n",
      "10776.82 1\n",
      "10780.71 1\n",
      "10788.67 1\n",
      "10793.11 1\n",
      "10801.37 1\n",
      "10806.13 1\n",
      "10807.46 1\n",
      "10814.59 1\n",
      "10827.96 1\n",
      "10850.78 1\n",
      "10863.05 1\n",
      "10877.81 1\n",
      "10919.81 1\n",
      "10921.54 1\n",
      "10930.16 1\n",
      "10938.11 1\n",
      "10944.65 1\n",
      "10960.52 1\n",
      "10963.5 1\n",
      "10965.95 1\n",
      "10976.03 1\n",
      "10978.16 1\n",
      "10989.43 1\n",
      "10999.05 1\n",
      "11008.0 1\n",
      "11040.16 1\n",
      "11047.43 1\n",
      "11066.8 1\n",
      "11079.61 1\n",
      "11082.13 1\n",
      "11091.21 1\n",
      "11093.87 1\n",
      "11111.59 1\n",
      "11119.11 1\n",
      "11128.95 1\n",
      "11144.51 1\n",
      "11145.95 1\n",
      "11158.5 1\n",
      "11159.46 1\n",
      "11170.06 1\n",
      "11177.08 1\n",
      "11185.62 1\n",
      "11187.93 1\n",
      "11197.42 1\n",
      "11204.23 1\n",
      "11222.48 1\n",
      "11225.96 1\n",
      "11226.88 1\n",
      "11227.39 1\n",
      "11227.48 1\n",
      "11245.38 1\n",
      "11254.38 1\n",
      "11255.38 1\n",
      "11274.46 1\n",
      "11274.98 1\n",
      "11294.26 1\n",
      "11294.45 1\n",
      "11297.57 1\n",
      "11306.1 1\n",
      "11309.18 1\n",
      "11316.41 1\n",
      "11316.69 1\n",
      "11321.2 1\n",
      "11331.58 1\n",
      "11334.3 1\n",
      "11335.97 1\n",
      "11338.94 1\n",
      "11343.33 1\n",
      "11360.99 1\n",
      "11362.95 1\n",
      "11370.28 1\n",
      "11375.25 1\n",
      "11386.56 1\n",
      "11391.46 1\n",
      "11401.42 1\n",
      "11401.63 1\n",
      "11416.56 1\n",
      "11424.0 1\n",
      "11430.75 1\n",
      "11434.61 1\n",
      "11473.38 1\n",
      "11486.53 1\n",
      "11490.75 1\n",
      "11517.19 1\n",
      "11520.18 1\n",
      "11523.63 1\n",
      "11527.08 1\n",
      "11537.69 1\n",
      "11538.57 1\n",
      "11540.86 1\n",
      "11550.29 1\n",
      "11553.65 1\n",
      "11555.21 1\n",
      "11555.42 1\n",
      "11575.37 1\n",
      "11575.55 1\n",
      "11575.84 1\n",
      "11580.56 1\n",
      "11584.59 1\n",
      "11585.16 1\n",
      "11597.38 1\n",
      "11599.27 1\n",
      "11602.11 1\n",
      "11611.28 1\n",
      "11611.51 1\n",
      "11621.67 1\n",
      "11623.42 1\n",
      "11628.1 1\n",
      "11654.34 1\n",
      "11655.82 1\n",
      "11667.01 1\n",
      "11670.32 1\n",
      "11670.73 1\n",
      "11674.12 1\n",
      "11680.91 1\n",
      "11687.29 1\n",
      "11695.04 1\n",
      "11699.26 1\n",
      "11715.72 1\n",
      "11738.88 1\n",
      "11740.0 1\n",
      "11743.24 1\n",
      "11756.06 1\n",
      "11758.69 1\n",
      "11764.35 1\n",
      "11768.65 1\n",
      "11768.71 1\n",
      "11775.75 1\n",
      "11783.81 1\n",
      "11789.33 1\n",
      "11794.91 1\n",
      "11799.72 1\n",
      "11803.58 1\n",
      "11814.59 1\n",
      "11828.78 1\n",
      "11830.36 1\n",
      "11832.86 1\n",
      "11849.13 1\n",
      "11856.28 1\n",
      "11873.76 1\n",
      "11887.39 1\n",
      "11912.13 1\n",
      "11916.22 1\n",
      "11916.26 1\n",
      "11945.5 1\n",
      "11968.78 1\n",
      "11971.75 1\n",
      "12008.34 1\n",
      "12010.74 1\n",
      "12019.93 1\n",
      "12020.62 1\n",
      "12043.27 1\n",
      "12056.18 1\n",
      "12068.39 1\n",
      "12078.39 1\n",
      "12086.92 1\n",
      "12104.25 1\n",
      "12107.03 1\n",
      "12112.53 1\n",
      "12115.91 1\n",
      "12122.19 1\n",
      "12142.99 1\n",
      "12147.83 1\n",
      "12154.25 1\n",
      "12155.91 1\n",
      "12192.94 1\n",
      "12200.31 1\n",
      "12200.96 1\n",
      "12217.95 1\n",
      "12227.39 1\n",
      "12247.91 1\n",
      "12249.96 1\n",
      "12252.57 1\n",
      "12254.75 1\n",
      "12261.79 1\n",
      "12264.68 1\n",
      "12277.9 1\n",
      "12284.58 1\n",
      "12295.95 1\n",
      "12300.94 1\n",
      "12309.23 1\n",
      "12309.39 1\n",
      "12328.03 1\n",
      "12357.31 1\n",
      "12361.42 1\n",
      "12374.97 1\n",
      "12376.37 1\n",
      "12378.57 1\n",
      "12397.49 1\n",
      "12398.62 1\n",
      "12400.33 1\n",
      "12401.58 1\n",
      "12403.6 1\n",
      "12403.81 1\n",
      "12410.67 1\n",
      "12453.72 1\n",
      "12455.38 1\n",
      "12455.75 1\n",
      "12458.8 1\n",
      "12479.72 1\n",
      "12495.15 1\n",
      "12502.81 1\n",
      "12520.53 1\n",
      "12524.68 1\n",
      "12531.39 1\n",
      "12542.76 1\n",
      "12547.91 1\n",
      "12567.53 1\n",
      "12638.51 1\n",
      "12654.35 1\n",
      "12659.88 1\n",
      "12668.26 1\n",
      "12680.46 1\n",
      "12681.35 1\n",
      "12682.9 1\n",
      "12691.45 1\n",
      "12711.15 1\n",
      "12713.9 1\n",
      "12721.55 1\n",
      "12760.01 1\n",
      "12766.24 1\n",
      "12772.72 1\n",
      "12780.22 1\n",
      "12783.21 1\n",
      "12788.37 1\n",
      "12800.82 1\n",
      "12810.16 1\n",
      "12813.91 1\n",
      "12835.19 1\n",
      "12849.1 1\n",
      "12871.79 1\n",
      "12873.63 1\n",
      "12879.7 1\n",
      "12884.75 1\n",
      "12888.85 1\n",
      "12893.19 1\n",
      "12893.47 1\n",
      "12913.55 1\n",
      "12919.92 1\n",
      "12925.64 1\n",
      "12936.64 1\n",
      "12941.12 1\n",
      "12975.35 1\n",
      "12978.81 1\n",
      "12983.26 1\n",
      "12987.28 1\n",
      "12989.06 1\n",
      "13019.89 1\n",
      "13029.61 1\n",
      "13035.91 1\n",
      "13036.52 1\n",
      "13074.81 1\n",
      "13078.28 1\n",
      "13081.64 1\n",
      "13091.92 1\n",
      "13100.75 1\n",
      "13116.73 1\n",
      "13118.56 1\n",
      "13122.89 1\n",
      "13135.26 1\n",
      "13157.08 1\n",
      "13165.68 1\n",
      "13173.8 1\n",
      "13173.82 1\n",
      "13184.51 1\n",
      "13189.35 1\n",
      "13190.3 1\n",
      "13240.01 1\n",
      "13247.31 1\n",
      "13255.05 1\n",
      "13268.55 1\n",
      "13277.82 1\n",
      "13296.89 1\n",
      "13298.38 1\n",
      "13308.01 1\n",
      "13314.19 1\n",
      "13359.2 1\n",
      "13365.66 1\n",
      "13366.66 1\n",
      "13368.21 1\n",
      "13383.25 1\n",
      "13460.92 1\n",
      "13472.79 1\n",
      "13473.35 1\n",
      "13480.91 1\n",
      "13498.28 1\n",
      "13510.96 1\n",
      "13532.85 1\n",
      "13547.39 1\n",
      "13574.25 1\n",
      "13574.53 1\n",
      "13597.5 1\n",
      "13630.93 1\n",
      "13650.64 1\n",
      "13655.59 1\n",
      "13672.54 1\n",
      "13680.25 1\n",
      "13717.41 1\n",
      "13725.55 1\n",
      "13731.45 1\n",
      "13743.12 1\n",
      "13743.27 1\n",
      "13743.42 1\n",
      "13762.79 1\n",
      "13896.99 1\n",
      "13915.9 1\n",
      "13976.64 1\n",
      "13987.59 1\n",
      "14003.88 1\n",
      "14023.22 1\n",
      "14036.28 1\n",
      "14062.6 1\n",
      "14069.41 1\n",
      "14080.95 1\n",
      "14099.03 1\n",
      "14124.0 1\n",
      "14154.26 1\n",
      "14155.97 1\n",
      "14160.05 1\n",
      "14216.37 1\n",
      "14241.75 1\n",
      "14242.7 1\n",
      "14317.2 1\n",
      "14361.38 1\n",
      "14383.36 1\n",
      "14398.89 1\n",
      "14425.74 1\n",
      "14429.41 1\n",
      "14595.51 1\n",
      "14664.0 1\n",
      "14669.61 1\n",
      "14693.45 1\n",
      "14715.53 1\n",
      "14722.35 1\n",
      "14738.09 1\n",
      "14838.84 1\n",
      "15005.43 1\n",
      "15070.32 1\n",
      "15188.65 1\n",
      "15423.03 1\n",
      "15509.97 1\n",
      "15516.52 1\n",
      "15571.26 1\n",
      "15858.91 1\n",
      "15878.11 1\n",
      "16371.42 1\n",
      "16838.94 1\n",
      "16955.76 1\n",
      "18026.01 1\n",
      "3263.0 1\n",
      "3676.68 1\n",
      "3689.95 1\n",
      "3825.7 1\n",
      "4111.4 1\n",
      "4316.73 1\n",
      "4492.44 1\n",
      "4523.91 1\n",
      "4690.57 1\n",
      "4711.89 1\n",
      "4762.81 1\n",
      "4771.65 1\n",
      "4863.73 1\n",
      "4992.6 1\n",
      "4994.48 1\n",
      "5002.58 1\n",
      "5024.52 1\n",
      "5039.46 1\n",
      "5191.08 1\n",
      "5192.38 1\n",
      "5200.06 1\n",
      "5218.49 1\n",
      "5304.6 1\n",
      "5347.74 1\n",
      "5353.97 1\n",
      "5387.41 1\n",
      "5387.75 1\n",
      "5447.16 1\n",
      "5515.09 1\n",
      "5570.45 1\n",
      "5622.5 1\n",
      "5647.92 1\n",
      "5695.21 1\n",
      "5720.98 1\n",
      "5738.82 1\n",
      "5756.12 1\n",
      "5805.53 1\n",
      "5830.99 1\n",
      "5846.65 1\n",
      "5900.78 1\n",
      "5907.07 1\n",
      "5914.3 1\n",
      "5977.46 1\n",
      "6065.64 1\n",
      "6081.5 1\n",
      "6098.15 1\n",
      "6114.85 1\n",
      "6127.84 1\n",
      "6131.92 1\n",
      "6168.66 1\n",
      "6172.23 1\n",
      "6204.86 1\n",
      "6244.75 1\n",
      "6283.67 1\n",
      "6298.75 1\n",
      "6330.43 1\n",
      "6351.79 1\n",
      "6362.93 1\n",
      "6367.22 1\n",
      "6398.91 1\n",
      "6402.3 1\n",
      "6406.38 1\n",
      "6411.25 1\n",
      "6447.99 1\n",
      "6452.83 1\n",
      "6461.86 1\n",
      "6471.68 1\n",
      "6495.01 1\n",
      "6498.85 1\n",
      "6517.22 1\n",
      "6517.93 1\n",
      "6523.74 1\n",
      "6535.91 1\n",
      "6569.87 1\n",
      "6574.25 1\n",
      "6590.63 1\n",
      "6623.97 1\n",
      "6635.19 1\n",
      "6638.87 1\n",
      "6662.59 1\n",
      "6670.37 1\n",
      "6681.61 1\n",
      "6683.82 1\n",
      "6715.23 1\n",
      "6744.87 1\n",
      "6749.49 1\n",
      "6773.23 1\n",
      "6783.99 1\n",
      "6805.48 1\n",
      "6857.29 1\n",
      "6930.95 1\n",
      "6977.56 1\n",
      "6992.09 1\n",
      "7026.18 1\n",
      "7062.59 1\n",
      "7073.61 1\n",
      "7077.34 1\n",
      "7097.52 1\n",
      "7158.66 1\n",
      "7170.66 1\n",
      "7178.92 1\n",
      "7189.31 1\n",
      "7193.84 1\n",
      "7222.35 1\n",
      "7232.06 1\n",
      "7239.49 1\n",
      "7277.27 1\n",
      "7287.57 1\n",
      "7320.49 1\n",
      "7324.32 1\n",
      "7325.93 1\n",
      "7342.14 1\n",
      "7351.38 1\n",
      "7351.72 1\n",
      "7361.92 1\n",
      "7362.18 1\n",
      "7364.12 1\n",
      "7380.48 1\n",
      "7396.1 1\n",
      "7424.11 1\n",
      "7460.05 1\n",
      "7464.08 1\n",
      "7490.92 1\n",
      "7491.55 1\n",
      "7492.9 1\n",
      "7495.6 1\n",
      "7504.79 1\n",
      "7529.23 1\n",
      "7599.36 1\n",
      "7605.49 1\n",
      "7636.98 1\n",
      "7638.13 1\n",
      "7642.2 1\n",
      "7656.02 1\n",
      "7673.0 1\n",
      "7675.52 1\n",
      "7683.97 1\n",
      "7684.02 1\n",
      "7686.13 1\n",
      "7713.12 1\n",
      "7720.61 1\n",
      "7730.53 1\n",
      "7736.13 1\n",
      "7737.57 1\n",
      "7748.34 1\n",
      "7750.54 1\n",
      "7759.15 1\n",
      "7761.2 1\n",
      "7762.73 1\n",
      "7777.37 1\n",
      "7778.73 1\n",
      "7810.06 1\n",
      "7814.68 1\n",
      "7818.13 1\n",
      "7819.46 1\n",
      "7837.67 1\n",
      "7864.63 1\n",
      "7876.84 1\n",
      "7896.65 1\n",
      "7904.67 1\n",
      "7915.44 1\n",
      "7948.18 1\n",
      "7960.64 1\n",
      "7961.21 1\n",
      "7982.38 1\n",
      "7985.76 1\n",
      "7996.78 1\n",
      "8007.18 1\n",
      "8010.76 1\n",
      "8011.38 1\n",
      "8021.05 1\n",
      "8024.08 1\n",
      "8042.76 1\n",
      "8046.4 1\n",
      "8058.73 1\n",
      "8066.94 1\n",
      "8083.76 1\n",
      "8092.13 1\n",
      "8092.55 1\n",
      "8093.03 1\n",
      "8097.01 1\n",
      "8099.29 1\n",
      "8100.43 1\n",
      "8122.34 1\n",
      "8131.64 1\n",
      "8143.38 1\n",
      "8185.22 1\n",
      "8190.27 1\n",
      "8193.77 1\n",
      "8198.06 1\n",
      "8207.1 1\n",
      "8213.41 1\n",
      "8218.63 1\n",
      "8232.11 1\n",
      "8233.25 1\n",
      "8235.74 1\n",
      "8243.28 1\n",
      "8261.89 1\n",
      "8268.74 1\n",
      "8270.52 1\n",
      "8271.02 1\n",
      "8274.32 1\n",
      "8280.03 1\n",
      "8283.32 1\n",
      "8284.89 1\n",
      "8286.87 1\n",
      "8291.93 1\n",
      "8297.87 1\n",
      "8316.89 1\n",
      "8338.05 1\n",
      "8370.15 1\n",
      "8375.73 1\n",
      "8389.84 1\n",
      "8397.41 1\n",
      "8403.02 1\n",
      "8403.78 1\n",
      "8418.81 1\n",
      "8427.12 1\n",
      "8429.65 1\n",
      "8465.53 1\n",
      "8473.65 1\n",
      "8474.11 1\n",
      "8475.8 1\n",
      "8480.93 1\n",
      "8502.52 1\n",
      "8513.75 1\n",
      "8514.24 1\n",
      "8514.8 1\n",
      "8519.64 1\n",
      "8523.04 1\n",
      "8530.23 1\n",
      "8556.73 1\n",
      "8558.52 1\n",
      "8560.76 1\n",
      "8563.24 1\n",
      "8575.71 1\n",
      "8580.85 1\n",
      "8583.5 1\n",
      "8588.46 1\n",
      "8595.24 1\n",
      "8601.01 1\n",
      "8603.6 1\n",
      "8610.43 1\n",
      "8614.75 1\n",
      "8615.52 1\n",
      "8617.98 1\n",
      "8625.96 1\n",
      "8626.48 1\n",
      "8628.8 1\n",
      "8642.01 1\n",
      "8644.07 1\n",
      "8646.2 1\n",
      "8646.72 1\n",
      "8662.18 1\n",
      "8665.79 1\n",
      "8667.02 1\n",
      "8670.98 1\n",
      "8677.28 1\n",
      "8677.62 1\n",
      "8688.17 1\n",
      "8688.21 1\n",
      "8736.06 1\n",
      "8748.36 1\n",
      "8755.02 1\n",
      "8771.02 1\n",
      "8772.0 1\n",
      "8772.26 1\n",
      "8783.13 1\n",
      "8787.39 1\n",
      "8801.13 1\n",
      "8801.86 1\n",
      "8815.24 1\n",
      "8817.22 1\n",
      "8817.29 1\n",
      "8818.03 1\n",
      "8819.13 1\n",
      "8824.84 1\n",
      "8825.87 1\n",
      "8828.59 1\n",
      "8829.83 1\n",
      "8840.51 1\n",
      "8841.3 1\n",
      "8863.36 1\n",
      "8871.53 1\n",
      "8874.83 1\n",
      "8878.96 1\n",
      "8880.17 1\n",
      "8884.39 1\n",
      "8907.17 1\n",
      "8907.76 1\n",
      "8911.91 1\n",
      "8914.46 1\n",
      "8917.57 1\n",
      "8918.52 1\n",
      "8929.88 1\n",
      "8930.49 1\n",
      "8931.09 1\n",
      "8934.99 1\n",
      "8939.61 1\n",
      "8941.92 1\n",
      "8948.86 1\n",
      "8950.33 1\n",
      "8951.42 1\n",
      "8972.54 1\n",
      "8986.28 1\n",
      "8988.67 1\n",
      "8996.04 1\n",
      "9000.05 1\n",
      "9000.31 1\n",
      "9000.36 1\n",
      "9004.44 1\n",
      "9014.9 1\n",
      "9034.21 1\n",
      "9036.27 1\n",
      "9060.28 1\n",
      "9063.16 1\n",
      "9065.28 1\n",
      "9087.9 1\n",
      "9090.43 1\n",
      "9091.21 1\n",
      "9091.99 1\n",
      "9101.71 1\n",
      "9133.36 1\n",
      "9139.73 1\n",
      "9148.74 1\n",
      "9166.22 1\n",
      "9171.56 1\n",
      "9182.81 1\n",
      "9191.02 1\n",
      "9191.58 1\n",
      "9200.69 1\n",
      "9214.98 1\n",
      "9219.4 1\n",
      "9219.95 1\n",
      "9228.84 1\n",
      "9229.96 1\n",
      "9248.87 1\n",
      "9261.41 1\n",
      "9265.59 1\n",
      "9270.9 1\n",
      "9282.33 1\n",
      "9282.99 1\n",
      "9303.36 1\n",
      "9303.43 1\n",
      "9306.05 1\n",
      "9315.6 1\n",
      "9317.12 1\n",
      "9322.44 1\n",
      "9324.49 1\n",
      "9347.47 1\n",
      "9347.89 1\n",
      "9349.99 1\n",
      "9351.88 1\n",
      "9353.38 1\n",
      "9365.92 1\n",
      "9378.24 1\n",
      "9381.12 1\n",
      "9399.92 1\n",
      "9401.99 1\n",
      "9403.58 1\n",
      "9408.96 1\n",
      "9419.66 1\n",
      "9435.2 1\n",
      "9446.84 1\n",
      "9451.03 1\n",
      "9459.52 1\n",
      "9461.83 1\n",
      "9472.72 1\n",
      "9499.3 1\n",
      "9516.87 1\n",
      "9519.64 1\n",
      "9521.35 1\n",
      "9528.99 1\n",
      "9541.45 1\n",
      "9552.57 1\n",
      "9557.2 1\n",
      "9574.64 1\n",
      "9574.89 1\n",
      "9576.07 1\n",
      "9577.42 1\n",
      "9593.71 1\n",
      "9598.03 1\n",
      "9601.95 1\n",
      "9608.45 1\n",
      "9613.84 1\n",
      "9617.59 1\n",
      "9621.04 1\n",
      "9624.18 1\n",
      "9625.89 1\n",
      "9635.87 1\n",
      "9641.61 1\n",
      "9649.26 1\n",
      "9655.25 1\n",
      "9655.42 1\n",
      "9672.03 1\n",
      "9683.85 1\n",
      "9703.93 1\n",
      "9706.88 1\n",
      "9714.47 1\n",
      "9721.59 1\n",
      "9722.23 1\n",
      "9722.92 1\n",
      "9723.71 1\n",
      "9735.41 1\n",
      "9737.34 1\n",
      "9741.19 1\n",
      "9743.57 1\n",
      "9764.39 1\n",
      "9766.95 1\n",
      "9771.22 1\n",
      "9772.95 1\n",
      "9773.22 1\n",
      "9779.12 1\n",
      "9782.83 1\n",
      "9788.78 1\n",
      "9793.42 1\n",
      "9805.7 1\n",
      "9814.22 1\n",
      "9815.03 1\n",
      "9826.29 1\n",
      "9844.79 1\n",
      "9845.35 1\n",
      "9848.4 1\n",
      "9852.25 1\n",
      "9853.64 1\n",
      "9855.18 1\n",
      "9874.69 1\n",
      "9879.37 1\n",
      "9885.12 1\n",
      "9891.3 1\n",
      "9893.92 1\n",
      "9903.78 1\n",
      "9918.77 1\n",
      "9919.38 1\n",
      "9921.89 1\n",
      "9922.3 1\n",
      "9923.79 1\n",
      "9940.54 1\n",
      "9940.76 1\n",
      "9951.97 1\n",
      "9957.26 1\n",
      "9982.55 1\n",
      "9988.68 1\n",
      "9993.5 1\n",
      "Total_Purchase 1\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Big Data ML Class.\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"churn.csv\")\n",
    "ratings = lines.map(lambda x: x.split(\",\")[3])\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0ce5d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 467\n",
      "1 433\n",
      "Account_Manager 1\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Big Data ML Class.\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"churn.csv\")\n",
    "ratings = lines.map(lambda x: x.split(\",\")[4])\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7b32b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 750\n",
      "1 150\n",
      "Churn 1\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Big Data ML Class.\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(\"churn.csv\")\n",
    "ratings = lines.map(lambda x: x.split(\",\")[7])\n",
    "result = ratings.countByValue()\n",
    "sortedResults = collections.OrderedDict(sorted(result.items()))\n",
    "for key, value in sortedResults.items():\n",
    "    print(\"%s %i\" % (key, value))\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eff01fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/01 00:53:10 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\n",
      "ValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/09/01 00:53:10 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.37 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n",
      "    yield next(iterator)\n",
      "  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\n",
      "ValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/09/01 00:53:10 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.37 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\nValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\nValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Covert this RDD to a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcolNames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Note there are lots of cases where you can avoid going from an RDDmto a DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mtoDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoDF\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    892\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             )\n\u001b[0;32m--> 894\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         )\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \"\"\"\n\u001b[1;32m    599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \"\"\"\n\u001b[0;32m--> 546\u001b[0;31m         \u001b[0mfirst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The first row in RDD is empty, \"\u001b[0m \u001b[0;34m\"can not infer schema\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1901\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1902\u001b[0m         \"\"\"\n\u001b[0;32m-> 1903\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1904\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1905\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.1.37 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\nValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 678, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 273, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/rdd.py\", line 1877, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n  File \"/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_25879/3290989384.py\", line 13, in <lambda>\nValueError: could not convert string to float: '1.000000000000000000e+00 -1.003975938886225322e+03'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Create a Sparksession (Note, the config section is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\").appName(\"Classification\").getOrCreate()\n",
    "\n",
    "# Load up our data and convert it to the format MLLİB expects.\n",
    "inputLines = spark.sparkContext.textFile(\"np.txt\")\n",
    "data = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]),Vectors.dense(float(x[1]))))\n",
    "\n",
    "# Covert this RDD to a DataFrame\n",
    "colNames = [\"label\",\"features\"]\n",
    "df = data.toDF(colNames)\n",
    "\n",
    "# Note there are lots of cases where you can avoid going from an RDDmto a DataFrame.\n",
    "# perhaps you are importing data from a real database. Or you are using structured streaming to get your data.\n",
    "\n",
    "# Let's split our data into training data and testing data\n",
    "trainTest = df.randomSplit([0.5, 0.5])\n",
    "trainingDF = trainTest[0]\n",
    "testDF = trainTest[1]\n",
    "\n",
    "# Now create our linear regression model\n",
    "gbtc = GBTClassifier(labelCol=\"label\", maxIter=10)\n",
    "model = gbtc.fit(trainingDF)\n",
    "\n",
    "\n",
    "\n",
    "# Now see if we can predict values in our test data.\n",
    "# Generate predictions using our linear regression model for all feature in our\n",
    "# Test dataframe:\n",
    "fullPredictions = model.transform(testDF).cache()\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels.\n",
    "predictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip them together\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out the predicted and actual values for each point\n",
    "for prediction in predictionAndLabel:\n",
    "    print(prediction)\n",
    "    \n",
    "# Stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "42efa960",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6bc264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
