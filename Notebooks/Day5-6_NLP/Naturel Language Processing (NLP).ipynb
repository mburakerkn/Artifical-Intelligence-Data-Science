{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499df06e",
   "metadata": {},
   "source": [
    "# Naturel Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97dfbd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import nltk \n",
    "from nltk.tokenize import sent_tokenize #paragraftan cümleleri ayırıp liste haline getirir.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6b317e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb55a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mehmetburakerkan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7b3acb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Welcome readers.', 'I hope you find interesting.', 'Please do reply.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Welcome readers. I hope you find interesting. Please do reply.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d79b73a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PierreVinken,59', 'years', 'old', ',', 'will', 'join', 'as', 'a', 'nonexecutive', 'director', 'on', 'Nov.', '29', '.']\n"
     ]
    }
   ],
   "source": [
    "# word tokenize \n",
    "text2=nltk.word_tokenize(\"PierreVinken,59 years old,will join as a nonexecutive director on Nov. 29 .\")\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b9fef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mehmetburakerkan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# türkçe\n",
    "WPT = nltk.WordPunctTokenizer()\n",
    "nltk.download(\"stopwords\")\n",
    "stop_word_list = nltk.corpus.stopwords.words(\"turkish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ca2f44",
   "metadata": {},
   "source": [
    "# Stemmer and Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3439a61",
   "metadata": {},
   "source": [
    "# Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73bb1672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85de12c",
   "metadata": {},
   "source": [
    "# Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5e22ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmerlan = LancasterStemmer()\n",
    "stemmerlan.stem(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b8c6a1",
   "metadata": {},
   "source": [
    "# Regexp Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0a58fe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n",
      "happiness\n",
      "pair\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp = RegexpStemmer(\"ing\")\n",
    "print(stemmerregexp.stem(\"working\"))\n",
    "print(stemmerregexp.stem(\"happiness\"))\n",
    "print(stemmerregexp.stem(\"pairing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39b8122",
   "metadata": {},
   "source": [
    "# Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54197a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n",
      "comiend\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "print(SnowballStemmer.languages)\n",
    "spanishstemmer = SnowballStemmer(\"spanish\")\n",
    "print(spanishstemmer.stem(\"comiende\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "754daa53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manag\n"
     ]
    }
   ],
   "source": [
    "frenchstemmer = SnowballStemmer(\"french\")\n",
    "print(frenchstemmer.stem(\"manager\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2afa9409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gelecek'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install TurkishStemmer\n",
    "from TurkishStemmer import TurkishStemmer\n",
    "turkishstemmer = TurkishStemmer()\n",
    "turkishstemmer.stem(\"geleceğim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70cad6",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e43fdf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mehmetburakerkan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9370e1",
   "metadata": {},
   "source": [
    "# POS-Part of Speech- isim,fiil,subject,proposition..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "221e3e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n",
      "work\n",
      "work\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "print(lemmatizer_output.lemmatize(\"working\"))\n",
    "\n",
    "# kelimenin fiil olduğunu soyluyoruz\n",
    "print(lemmatizer_output.lemmatize(\"working\",pos=\"v\"))\n",
    "print(lemmatizer_output.lemmatize(\"works\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5844a180",
   "metadata": {},
   "source": [
    "# Difference Between Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4521753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'men'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer_output = PorterStemmer()\n",
    "print(stemmer_output.stem(\"happiness\"))\n",
    "stemmer_output.stem(\"Mens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c3a4fdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mens'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_output = WordNetLemmatizer()\n",
    "lemmatizer_output.lemmatize(\"Mens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649eaeb",
   "metadata": {},
   "source": [
    "# POS Tagging and POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b747b2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Drive',\n",
       " 'into',\n",
       " 'NLTK',\n",
       " ':',\n",
       " 'Part-of-speech',\n",
       " 'tagging',\n",
       " 'and',\n",
       " 'POS',\n",
       " 'Tagger']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = word_tokenize(\"Drive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41b333d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mehmetburakerkan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef030ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Drive', 'NNP'),\n",
       " ('into', 'IN'),\n",
       " ('NLTK', 'NNP'),\n",
       " (':', ':'),\n",
       " ('Part-of-speech', 'JJ'),\n",
       " ('tagging', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('POS', 'NNP'),\n",
       " ('Tagger', 'NNP')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "649316ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beautiful', 'He is the man'), ('morning', 'He is the man')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import DefaultTagger\n",
    "tag = DefaultTagger(\"He is the man\")\n",
    "tag.tag([\"Beautiful\",\"morning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "61b86b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a763af1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import spell\n",
    "spell(\"Tghe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0408f756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /opt/anaconda3/lib/python3.9/site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f3044",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "454f0f3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "b = TextBlob(\"I havv good speling!\")\n",
    "#print(b.detect_language())\n",
    "print(b.correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dffe4d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fallibility', 0.3333333333333333),\n",
       " ('capability', 0.3333333333333333),\n",
       " ('affability', 0.3333333333333333)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word(\"falability\")\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88d92564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a5fea407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "de\n",
      "pt\n",
      "tr\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "print(detect(\"War doesn't show who's right , just who's left\"))\n",
    "print(detect(\"Ein, zwei, drei, vier\"))\n",
    "print(detect(\"Eu gosto de mulher\"))\n",
    "print(detect(\"Nasılsınız?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cac31cae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_1820/1409943547.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0men_blob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'I am a free black man loved by Jesus Christ.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0men_blob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ru\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/textblob/blob.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, from_lang, to)\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         )\n\u001b[0;32m--> 568\u001b[0;31m         return self.__class__(self.translator.translate(self.raw,\n\u001b[0m\u001b[1;32m    569\u001b[0m                               from_lang=from_lang, to_lang=to))\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/textblob/translate.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, source, from_lang, to_lang, host, type_)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/textblob/translate.py\u001b[0m in \u001b[0;36m_validate_translation\u001b[0;34m(self, source, result)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotTranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Translation API returned the input string unchanged.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "en_blob = TextBlob(u'I am a free black man loved by Jesus Christ.')\n",
    "en_blob.translate(to=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09625c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ee836515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Benim', 'NNP'), ('çok', 'NNP'), ('işim', 'NN'), ('var', 'NN')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_blob = TextBlob(u'Benim çok işim var.')\n",
    "tr_blob.tags\n",
    "#print(tr_blob.translate(to=\"ru\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564c51be",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912d1fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "simple_train = [\"Call you tonight\",\"Call me a cab\",\" please call me...PLEASE\"]\n",
    "\n",
    "vect = CountVectorizer()\n",
    "\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f885d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1,6),columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7660f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3e842dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(),columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62fbe8a",
   "metadata": {},
   "source": [
    "# CountVectorizer - Fit Transform with NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "62440028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this\n",
       "0    0         1      1   1    0       0    1      0     1\n",
       "1    0         1      0   1    0       1    1      0     1\n",
       "2    1         0      0   0    1       0    1      1     0\n",
       "3    0         1      1   1    0       0    1      0     1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)\n",
    "\n",
    "corpus = [\"This is the first document\",\"This is the second document\",\"And the third one\",\"Is this the first document?\"]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tf = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5d6a14a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 4)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981445dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5413fdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp = pd.read_csv(\"yelp.csv\")\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "acfc1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   business_id  10000 non-null  object\n",
      " 1   date         10000 non-null  object\n",
      " 2   review_id    10000 non-null  object\n",
      " 3   stars        10000 non-null  int64 \n",
      " 4   text         10000 non-null  object\n",
      " 5   type         10000 non-null  object\n",
      " 6   user_id      10000 non-null  object\n",
      " 7   cool         10000 non-null  int64 \n",
      " 8   useful       10000 non-null  int64 \n",
      " 9   funny        10000 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "yelp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71288e",
   "metadata": {},
   "source": [
    "- Bütün cümleler küçük harfe çevrilecek\n",
    "- Noktalama işaretlerini kaldır\n",
    "- Rakamları kaldır\n",
    "- Satır sonu , \\n \\r\n",
    "- Stop words leri kaldır(gereksiz kelimeler)\n",
    "- Tokenize et\n",
    "- Lemma Stemma (ekleri kaldır, kökleri bul)\n",
    "- Vetorizer ile yazıları rakama atıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc83ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[\"text\"]=yelp[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4caeeb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_7114/3541075574.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  yelp[\"text\"]=yelp[\"text\"].str.replace(\"[^\\w\\s]\",\"\")\n"
     ]
    }
   ],
   "source": [
    "yelp[\"text\"]=yelp[\"text\"].str.replace(\"[^\\w\\s]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea5c467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/_tmyvw0x5yz487l03fhlkrsw0000gn/T/ipykernel_7114/3682925849.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  yelp[\"text\"]=yelp[\"text\"].str.replace(\"\\d+\",\"\")\n"
     ]
    }
   ],
   "source": [
    "yelp[\"text\"]=yelp[\"text\"].str.replace(\"\\d+\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b501acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp[\"text\"]=yelp[\"text\"].str.replace(\"\\n\",\" \").replace(\"\\r\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a327092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4086,)\n"
     ]
    }
   ],
   "source": [
    "yelp_best_worst = yelp[(yelp.stars==5)|(yelp.stars==1)]\n",
    "\n",
    "yelp_best_worst.reset_index(drop=True,inplace=True)\n",
    "\n",
    "x = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "x_train,  x_test, y_train, y_test = train_test_split(x,y, random_state=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36711c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       my wife took me here on my birthday for breakf...\n",
      "1       i have no idea why some people give bad review...\n",
      "2       rosie dakota and i love chaparral dog park its...\n",
      "3       general manager scott petello is a good egg no...\n",
      "4       drop what youre doing and drive here after i a...\n",
      "                              ...                        \n",
      "4081    yes i do rock the hipster joints  i dig this p...\n",
      "4082    only  stars   a few notes the folks that rated...\n",
      "4083    im not normally one to jump at reviewing a cha...\n",
      "4084    lets seewhat is there not to like about surpri...\n",
      "4085     locations all  star average i think arizona r...\n",
      "Name: text, Length: 4086, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b4e46",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f0ccc0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5877)\t1\n",
      "  (0, 13267)\t1\n",
      "  (0, 17839)\t2\n",
      "  (0, 10102)\t1\n",
      "  (0, 11893)\t1\n",
      "  (0, 7711)\t1\n",
      "  (0, 4864)\t1\n",
      "  (0, 17363)\t1\n",
      "  (0, 13323)\t2\n",
      "  (0, 2631)\t1\n",
      "  (0, 15871)\t1\n",
      "  (0, 3608)\t1\n",
      "  (0, 727)\t1\n",
      "  (0, 2089)\t2\n",
      "  (0, 6748)\t4\n",
      "  (0, 17026)\t1\n",
      "  (0, 9348)\t1\n",
      "  (0, 3141)\t1\n",
      "  (0, 1315)\t1\n",
      "  (0, 2087)\t1\n",
      "  (0, 15732)\t1\n",
      "  (0, 13295)\t1\n",
      "  (0, 1281)\t1\n",
      "  (0, 3119)\t1\n",
      "  (0, 14368)\t1\n",
      "  :\t:\n",
      "  (3062, 6800)\t1\n",
      "  (3062, 7011)\t1\n",
      "  (3062, 9129)\t1\n",
      "  (3063, 15732)\t1\n",
      "  (3063, 12811)\t1\n",
      "  (3063, 32)\t1\n",
      "  (3063, 9942)\t1\n",
      "  (3063, 6129)\t2\n",
      "  (3063, 9804)\t1\n",
      "  (3063, 8368)\t1\n",
      "  (3063, 8588)\t2\n",
      "  (3063, 7441)\t1\n",
      "  (3063, 2661)\t1\n",
      "  (3063, 2571)\t1\n",
      "  (3063, 15869)\t1\n",
      "  (3063, 6928)\t1\n",
      "  (3063, 17828)\t1\n",
      "  (3063, 7211)\t1\n",
      "  (3063, 15084)\t1\n",
      "  (3063, 5206)\t1\n",
      "  (3063, 4557)\t1\n",
      "  (3063, 11483)\t1\n",
      "  (3063, 16046)\t1\n",
      "  (3063, 17322)\t1\n",
      "  (3063, 6806)\t1\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "print(x_train_dtm)\n",
    "\n",
    "x_test_dtm = vect.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6e7531c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55d5e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "924544f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "46f6e7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1607    looking a cutting edge wanting the best for ev...\n",
      "3409    greatness in the form of food just like the ot...\n",
      "1751    the flower studio far exceeded my expectations...\n",
      "2275         so yummy strange combination but great place\n",
      "230     ive been hearing about these cheesecakes from ...\n",
      "                              ...                        \n",
      "2793    honey jalapeño chicken lollipops and sweet pot...\n",
      "671                      probably my favorite restaurant \n",
      "3441    a philosophical elder of my profession commonl...\n",
      "3224    first im sorry this review is lengthy but i re...\n",
      "3362    you speak italian to me and provide mouth wate...\n",
      "Name: text, Length: 1022, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "34e42a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>______</th>\n",
       "      <th>_______________</th>\n",
       "      <th>_c</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaamazing</th>\n",
       "      <th>aaammmazzing</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>zucca</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwiebelkräuter</th>\n",
       "      <th>éclairs</th>\n",
       "      <th>école</th>\n",
       "      <th>ém</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 18081 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ______  _______________  _c  aa  aaa  aaaamazing  aaammmazzing  aaron  ab  \\\n",
       "0       0                0   0   0    0           0             0      0   0   \n",
       "1       0                0   0   0    0           0             0      0   0   \n",
       "2       0                0   0   0    0           0             0      0   0   \n",
       "3       0                0   0   0    0           0             0      0   0   \n",
       "4       0                0   0   0    0           0             0      0   0   \n",
       "\n",
       "   abandoned  ...  zucca  zucchini  zuchinni  zumba  zupa  zuzu  \\\n",
       "0          0  ...      0         0         0      0     0     0   \n",
       "1          0  ...      0         0         0      0     0     0   \n",
       "2          0  ...      0         0         0      0     0     0   \n",
       "3          0  ...      0         0         0      0     0     0   \n",
       "4          0  ...      0         0         0      0     0     0   \n",
       "\n",
       "   zwiebelkräuter  éclairs  école  ém  \n",
       "0               0        0      0   0  \n",
       "1               0        0      0   0  \n",
       "2               0        0      0   0  \n",
       "3               0        0      0   0  \n",
       "4               0        0      0   0  \n",
       "\n",
       "[5 rows x 18081 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = pd.DataFrame(x_train_dtm.toarray(), columns=vect.get_feature_names())\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e2e59fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2790    fillybs  only  reviews nine now  wow do i miss...\n",
       "725     my husband and i absolutely love this restaura...\n",
       "1578    we went today after lunch i got my usual of li...\n",
       "282     totally dissapointed  i had purchased a coupon...\n",
       "2024    costco travel  my husband and i recently retur...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c45c508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 171153)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "982b71cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3064x171153 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 568619 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7b79f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zone', 'zone of', 'zone out', 'zone when', 'zones', 'zones dolls', 'zoning', 'zoning issues', 'zoo', 'zoo and', 'zoo is', 'zoo ive', 'zoo not', 'zoo the', 'zoyo', 'zoyo for', 'zucca', 'zucca appetizer', 'zucchini', 'zucchini and', 'zucchini bread', 'zucchini broccoli', 'zucchini carrots', 'zucchini fries', 'zucchini pieces', 'zucchini strips', 'zucchini veal', 'zucchini very', 'zucchini with', 'zuchinni', 'zuchinni again', 'zuchinni the', 'zumba', 'zumba class', 'zumba or', 'zumba yogalates', 'zupa', 'zupa flavors', 'zuzu', 'zuzu in', 'zuzu is', 'zuzu the', 'zwiebelkräuter', 'zwiebelkräuter salat', 'éclairs', 'éclairs napoleons', 'école', 'école lenôtre', 'ém', 'ém all']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfafac",
   "metadata": {},
   "source": [
    "# Predict the Star rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d60a6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168297455968689\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f294a",
   "metadata": {},
   "source": [
    "# Calculate Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "12891808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print(\"Features: \",x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9a7d0ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  18379\n",
      "Accuracy:  0.9168297455968689\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "022abc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  171153\n",
      "Accuracy:  0.8522504892367906\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86519d68",
   "metadata": {},
   "source": [
    "# Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09ba5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_corpus = \"I am a Sidy. A free black man. Believe it or not; it is true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "647221e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  18081\n",
      "Accuracy:  0.9168297455968689\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1d88eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'beforehand', 'further', 'move', 'amoungst', 'cant', 'any', 'one', 'over', 'up', 'anywhere', 'alone', 'cannot', 'con', 'put', 'made', 'together', 'no', 'fire', 'its', 'thereupon', 'ourselves', 'find', 'once', 'during', 'the', 'might', 'whence', 'name', 'cry', 'hereupon', 'and', 'can', 'hereafter', 'beyond', 'most', 'nothing', 'eleven', 'hence', 'show', 'serious', 'call', 'before', 'was', 'nine', 'on', 'would', 'being', 'both', 'meanwhile', 'should', 'those', 'down', 'except', 'de', 'a', 'whither', 'indeed', 'moreover', 'there', 'will', 'her', 'behind', 'everything', 'whereby', 'thru', 'himself', 'becomes', 'detail', 'among', 'go', 'beside', 'another', 'mine', 'front', 'not', 'toward', 'thick', 'they', 'full', 'couldnt', 'herself', 'you', 'two', 'with', 'get', 'though', 'sometimes', 'of', 'too', 'to', 'again', 'due', 'from', 'ltd', 'last', 'least', 'thereby', 'each', 're', 'thus', 'by', 'through', 'inc', 'done', 'be', 'their', 'under', 'same', 'at', 'few', 'some', 'noone', 'while', 'since', 'something', 'everywhere', 'four', 'an', 'sixty', 'all', 'myself', 'hers', 'whole', 'may', 'itself', 'around', 'empty', 'whether', 'therefore', 'co', 'rather', 'see', 'so', 'either', 'have', 'etc', 'within', 'less', 'sincere', 'this', 'elsewhere', 'own', 'twenty', 'always', 'many', 'every', 'thereafter', 'above', 'wherein', 'anyhow', 'yet', 'seeming', 'wherever', 'everyone', 'give', 'she', 'then', 'ours', 'or', 'whatever', 'towards', 'otherwise', 'hasnt', 'yourself', 'well', 'latterly', 'found', 'your', 'thence', 'others', 'back', 'across', 'enough', 'thin', 'what', 'out', 'system', 'whenever', 'anyone', 'part', 'about', 'been', 'never', 'when', 'it', 'eg', 'still', 'are', 'perhaps', 'for', 'seem', 'please', 'sometime', 'we', 'us', 'namely', 'such', 'is', 'fifteen', 'he', 'upon', 'him', 'else', 'throughout', 'someone', 'where', 'un', 'first', 'could', 'eight', 'because', 'per', 'after', 'fifty', 'between', 'as', 'twelve', 'becoming', 'bottom', 'none', 'ten', 'become', 'bill', 'somewhere', 'seems', 'much', 'third', 'below', 'but', 'amongst', 'herein', 'formerly', 'ever', 'whoever', 'my', 'than', 'only', 'has', 'hereby', 'describe', 'mill', 'interest', 'onto', 'these', 'whose', 'whom', 'whereas', 'his', 'nobody', 'whereupon', 'via', 'into', 'latter', 'which', 'nowhere', 'am', 'themselves', 'afterwards', 'me', 'therein', 'fill', 'other', 'nor', 'even', 'became', 'against', 'more', 'our', 'several', 'however', 'anything', 'if', 'somehow', 'side', 'very', 'who', 'must', 'six', 'why', 'amount', 'now', 'seemed', 'next', 'top', 'whereafter', 'forty', 'i', 'here', 'take', 'mostly', 'in', 'hundred', 'nevertheless', 'also', 'until', 'yourselves', 'anyway', 'often', 'without', 'besides', 'although', 'how', 'already', 'were', 'had', 'ie', 'almost', 'former', 'five', 'along', 'do', 'yours', 'off', 'neither', 'three', 'that', 'keep', 'them'})\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_stop_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1ea392f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  100000\n",
      "Accuracy:  0.8816046966731899\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),max_features=100000)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12a241db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  43256\n",
      "Accuracy:  0.9305283757338552\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),min_df=2)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdae24",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1052150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife took me here on my birthday for breakfast and it was excellent  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure  our waitress was excellent and our food arrived quickly on the semibusy saturday morning  it looked like the place fills up pretty quickly so the earlier you get here the better  do yourself a favor and get their bloody mary  it was phenomenal and simply the best ive ever had  im pretty sure they only use ingredients from their garden and blend them fresh when you order it  it was amazing  while everything on the menu looks excellent i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious  it came with  pieces of their griddled bread with was amazing and it absolutely made the meal complete  it was the best toast ive ever had  anyway i cant wait to go back\n"
     ]
    }
   ],
   "source": [
    "print(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1fcca551",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = TextBlob(yelp_best_worst.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7c64236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excellent', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'grounds', 'an', 'absolute', 'pleasure', 'our', 'waitress', 'was', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semibusy', 'saturday', 'morning', 'it', 'looked', 'like', 'the', 'place', 'fills', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloody', 'mary', 'it', 'was', 'phenomenal', 'and', 'simply', 'the', 'best', 'ive', 'ever', 'had', 'im', 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amazing', 'while', 'everything', 'on', 'the', 'menu', 'looks', 'excellent', 'i', 'had', 'the', 'white', 'truffle', 'scrambled', 'eggs', 'vegetable', 'skillet', 'and', 'it', 'was', 'tasty', 'and', 'delicious', 'it', 'came', 'with', 'pieces', 'of', 'their', 'griddled', 'bread', 'with', 'was', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'it', 'was', 'the', 'best', 'toast', 'ive', 'ever', 'had', 'anyway', 'i', 'cant', 'wait', 'to', 'go', 'back'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "467436dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"my wife took me here on my birthday for breakfast and it was excellent  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure  our waitress was excellent and our food arrived quickly on the semibusy saturday morning  it looked like the place fills up pretty quickly so the earlier you get here the better  do yourself a favor and get their bloody mary  it was phenomenal and simply the best ive ever had  im pretty sure they only use ingredients from their garden and blend them fresh when you order it  it was amazing  while everything on the menu looks excellent i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious  it came with  pieces of their griddled bread with was amazing and it absolutely made the meal complete  it was the best toast ive ever had  anyway i cant wait to go back\")]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "721e3f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"my wife took me here on my birthday for breakfast and it was excellent  the weather was perfect which made sitting outside overlooking their grounds an absolute pleasure  our waitress was excellent and our food arrived quickly on the semibusy saturday morning  it looked like the place fills up pretty quickly so the earlier you get here the better  do yourself a favor and get their bloody mary  it was phenomenal and simply the best ive ever had  im pretty sure they only use ingredients from their garden and blend them fresh when you order it  it was amazing  while everything on the menu looks excellent i had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious  it came with  pieces of their griddled bread with was amazing and it absolutely made the meal complete  it was the best toast ive ever had  anyway i cant wait to go back\")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c2cf49",
   "metadata": {},
   "source": [
    "# Steming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "def87c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'was', 'excel', 'the', 'weather', 'was', 'perfect', 'which', 'made', 'sit', 'outsid', 'overlook', 'their', 'ground', 'an', 'absolut', 'pleasur', 'our', 'waitress', 'was', 'excel', 'and', 'our', 'food', 'arriv', 'quick', 'on', 'the', 'semibusi', 'saturday', 'morn', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretti', 'quick', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloodi', 'mari', 'it', 'was', 'phenomen', 'and', 'simpli', 'the', 'best', 'ive', 'ever', 'had', 'im', 'pretti', 'sure', 'they', 'onli', 'use', 'ingredi', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'was', 'amaz', 'while', 'everyth', 'on', 'the', 'menu', 'look', 'excel', 'i', 'had', 'the', 'white', 'truffl', 'scrambl', 'egg', 'veget', 'skillet', 'and', 'it', 'was', 'tasti', 'and', 'delici', 'it', 'came', 'with', 'piec', 'of', 'their', 'griddl', 'bread', 'with', 'was', 'amaz', 'and', 'it', 'absolut', 'made', 'the', 'meal', 'complet', 'it', 'was', 'the', 'best', 'toast', 'ive', 'ever', 'had', 'anyway', 'i', 'cant', 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "print([stemmer.stem(word) for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3a6541ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'took', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'wa', 'excellent', 'the', 'weather', 'wa', 'perfect', 'which', 'made', 'sitting', 'outside', 'overlooking', 'their', 'ground', 'an', 'absolute', 'pleasure', 'our', 'waitress', 'wa', 'excellent', 'and', 'our', 'food', 'arrived', 'quickly', 'on', 'the', 'semibusy', 'saturday', 'morning', 'it', 'looked', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloody', 'mary', 'it', 'wa', 'phenomenal', 'and', 'simply', 'the', 'best', 'ive', 'ever', 'had', 'im', 'pretty', 'sure', 'they', 'only', 'use', 'ingredient', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'wa', 'amazing', 'while', 'everything', 'on', 'the', 'menu', 'look', 'excellent', 'i', 'had', 'the', 'white', 'truffle', 'scrambled', 'egg', 'vegetable', 'skillet', 'and', 'it', 'wa', 'tasty', 'and', 'delicious', 'it', 'came', 'with', 'piece', 'of', 'their', 'griddled', 'bread', 'with', 'wa', 'amazing', 'and', 'it', 'absolutely', 'made', 'the', 'meal', 'complete', 'it', 'wa', 'the', 'best', 'toast', 'ive', 'ever', 'had', 'anyway', 'i', 'cant', 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemmatize() for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a57ff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'wife', 'take', 'me', 'here', 'on', 'my', 'birthday', 'for', 'breakfast', 'and', 'it', 'be', 'excellent', 'the', 'weather', 'be', 'perfect', 'which', 'make', 'sit', 'outside', 'overlook', 'their', 'ground', 'an', 'absolute', 'pleasure', 'our', 'waitress', 'be', 'excellent', 'and', 'our', 'food', 'arrive', 'quickly', 'on', 'the', 'semibusy', 'saturday', 'morning', 'it', 'look', 'like', 'the', 'place', 'fill', 'up', 'pretty', 'quickly', 'so', 'the', 'earlier', 'you', 'get', 'here', 'the', 'better', 'do', 'yourself', 'a', 'favor', 'and', 'get', 'their', 'bloody', 'mary', 'it', 'be', 'phenomenal', 'and', 'simply', 'the', 'best', 'ive', 'ever', 'have', 'im', 'pretty', 'sure', 'they', 'only', 'use', 'ingredients', 'from', 'their', 'garden', 'and', 'blend', 'them', 'fresh', 'when', 'you', 'order', 'it', 'it', 'be', 'amaze', 'while', 'everything', 'on', 'the', 'menu', 'look', 'excellent', 'i', 'have', 'the', 'white', 'truffle', 'scramble', 'egg', 'vegetable', 'skillet', 'and', 'it', 'be', 'tasty', 'and', 'delicious', 'it', 'come', 'with', 'piece', 'of', 'their', 'griddle', 'bread', 'with', 'be', 'amaze', 'and', 'it', 'absolutely', 'make', 'the', 'meal', 'complete', 'it', 'be', 'the', 'best', 'toast', 'ive', 'ever', 'have', 'anyway', 'i', 'cant', 'wait', 'to', 'go', 'back']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemmatize(pos=\"v\") for word in review.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "02b34a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(text):\n",
    "    text = str(text).lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "55916c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  13313\n",
      "Accuracy:  0.9275929549902152\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(analyzer=split_into_lemmas)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c940cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yuuuummmma', 'yuuuuummmmmyyi', 'yuuuuuuum', 'yuyuyummi', 'yuzu', 'z', 'zach', 'zam', 'zanella', 'zankou', 'zappo', 'zatsiki', 'zen', 'zenlik', 'zenroad', 'zero', 'zerostar', 'zest', 'zexperi', 'zgrill', 'zha', 'zhou', 'zia', 'zilch', 'zin', 'zinburg', 'zinburgergeist', 'zinc', 'zinfandel', 'zing', 'zip', 'zipcar', 'zipp', 'zipper', 'ziti', 'zoe', 'zombi', 'zone', 'zoo', 'zoyo', 'zucca', 'zucchini', 'zuchinni', 'zumba', 'zupa', 'zuzu', 'zwiebelkräut', 'éclair', 'école', 'ém']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c330394",
   "metadata": {},
   "source": [
    "# Using TF-IDF to Summarize a Yelp Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0d38899d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 34834)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(stop_words=\"english\")\n",
    "dtm = vect.fit_transform(yelp.text)\n",
    "features = vect.get_feature_names()\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2310da3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def summarize():\n",
    "    review_length=0\n",
    "    while review_length < 300:\n",
    "        review_id = np.random.randint(0, len(yelp))\n",
    "        review_text = str(yelp.text[review_id])\n",
    "        review_length = len(review_text)\n",
    "        \n",
    "    word_scores = {}\n",
    "    for word in TextBlob(review_text).words:\n",
    "        word = word.lower()\n",
    "        if word in features:\n",
    "            word_scores[word] = dtm[review_id, features.index(word)]\n",
    "            \n",
    "            \n",
    "    print(\"Top Scoring Words: \")\n",
    "    top_scores = sorted(word_scores.items(),key=lambda x: x[1], reverse=True)[:5]\n",
    "    for word, score in top_scores:\n",
    "        print(word)\n",
    "        \n",
    "    print(\"\\n\"+ review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e22ab50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Scoring Words: \n",
      "olivos\n",
      "los\n",
      "mexican\n",
      "traditional\n",
      "north\n",
      "\n",
      "the best example north scottsdale has of traditional phoenix sonoran mexican food  los olivos restaurants have been around forever  at least since the s  the food is great if not fancy and the place is pretty free of gimmicks  you can get your fix of cheese crisps aka quesadillas for you ca transplants deep fried tacos chimichangas combination plates with so much sauce and cheese you cant tell whats underneath but it all works together to taste so good  los olivos still plops down the free chips and salsa right when you sit down so youre good an full when that combo plate comes and glad for it  the atmosphere is more like a nice mexican diner than mexican cantina and service is solid they often have mariachis playing on weekend nights  margaritas arent great but the beer is cold  dont bother with dessert  cheap eats for all the food especially since you wont be hungry for another  hours  if youre in north scottsdale and in the mood for traditional phoenix style mexican food los olivos is a great choice\n"
     ]
    }
   ],
   "source": [
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eae16c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
